\chapter{Discussion}
\label{cha:discussion}

There have been several caveats made in the explanation of the data collection, methodology, results, and analysis of results. The goal of this section is to summarize such information (involving a re-invoking of certain statements made in previous sections) and to build on that. Like the rest of the report structure, this section is split under the two different research questions investigated in this thesis, starting with the secondary goal of the report and finishing with the primary goals of the report.

\section{Foreground Removal}
\label{sec:fr_disc}

A major shortfall of the foreground subtraction investigation is the lack of statistical analysis, which will be a common theme throughout this section. In some cases, this is justified, as statistical analysis does not need to be performed especially when the method visually appears as adequate. At the same time, this investigation is limited from a lack of characterisation of the true RM sky, including simulations to predict the distribution of corrected RMs and high-quality data to interpolate with.


This report is not meant to act as "closing the book" on research into foreground subtraction techniques, but to point out important considerations to make as research moves into the SKA era, and to conduct a preliminary analysis of how to tackle these predicted problems. This comes in the explanation in section \ref{sec:intp}, which highlights how higher-definition data will inevitably result in issues involving foreground correction for objects of a particular angular size. Thus, the need to extend beyond interpolation techniques.


The NUFFT method is one example of a method that can be assessed visually, and without the need for statistical analysis. Figure \ref{fig:nufft} clearly shows that even carrying out an interpolation using a NUFFT requires a large sample size to be adequate. It would not be appropriate to completely write-off NUFFTs as an interpolation and bandpass technique, as with a more complete POSSUM dataset, and future SKA RM projects, millions of RM points would be adequate to interpolate and bandpass using a NUFFT. There will simply not be enough data in the few years after this paper.


Given that the Hutschenreuter map interpolates data that is at a spatial sampling frequency of 1 sampling point per degree squared, attempting to remove objects on the scale of 1 degree should not alter the interpolation at all. However, there are several factors that prevents this from being an accurate description of the Hutschenreuter map: H-alpha inclusion providing more detailed information; more RM sources being detected by telescopes close to the Galactic midplane; and distortion effects occurring on the edges of the cartesian map. The latter two can be accounted for by performing the filtering on a reduced form of the dataset that does not include extremes in Galactic latitude. Additionally, from figure \ref{fig:rm_scatter}, it is clear that RMs near the Galactic midplane can become quite scattered. Even though this is true for the Hutschenreuter map, it still is a demonstration of the scatter present near the Galactic midplane. However, filtering not performed with the statistical analysis of foreground removal techniques, as it is important to determine the effectiveness of a given method at accounting for these changes.


This is the justification for the use of a Pearson's R-squared statistic, as it can determine how similar the pre and post bandpass and convolution methods are at preserving the broader RM sky. With the expectation being that the R-squared statistic is close to unity but not necessarily exactly unity. One major downside is that there is no way to quantify the appropriate R-squared statistic that should be desired. Thus, only a high R-squared statistic is possible qualifier.


It is clear from the R-squared statistic that, even with a less-exact method of determining validity, only the Crosshatch-Bandpass method is viable as an interpolation correction method. It can be pointed out that it may be unfair to compare the Annulus-Convolved method to the raw interpolation. However, it is justified in the sense that the raw interpolation has been used as the primary method in past literature, and from figure \ref{fig:anisotropy}, it is demonstrably useful at removing the anisotropy produced by the ISM.


Both annulus methods had major points of failure regarding the specific technique that they relied on to correct the RMs. When the RM sky is meant to be full of objects and obstructions of different sizes and overlapping positions, it can make it difficult to use a fixed size or RM count to correct. The RM distributions in figure \ref{fig:all_hvcs_avg} show visually that a method involving the averages of a set of RMs, and corrections based on that average are possible, but the effective 'goldilocks zone' for a good size or sampling count is small and would require large simulations to find the ideal size – even then, it may not be adequate for certain objects.


The Crosshatch-Bandpass method seems to be the most appropriate as a first step in SKA-era foreground interpolation. However, with the rippling effects shown in figure \ref{fig:ripples}, there are improvements that can be made. The Fourier Transform of a top-hat function is inevitably going to introduce rippling. This may not be as much of the case with the Tukey or Gaussian windows, which is much more common in other signal processing analyses. The choice to reduce bandpass opacity is another option that clearly removes ripples, at the cost of dampening the intended goal of using FFT signal processing methods. Some combination of these may be useful for future research relying on RM-correction.


The interesting result provided in \cite{ID73} clearly still shows with a repeated analysis on the Hutschenreuter map, and the improved Crosshatch-Bandpass method. There is still little reason for why corrected RMs take this shape, or if this is meant to be the true distribution of RMs post ISM correction. It is rather crucial that research be conducted into what the true distribution of ISM-corrected RMs should be. It is trivial to understand why the distribution should be symmetrical and centred at zero, as extragalactic polarisation is effectively random and not biased based on sign. The Student's t-distribution is one of many different distributions satisfying these properties that are viable candidates for representing corrected RMs, it needs to be explained why this specific distribution is the one that RMs follow.


Under the assumption that it is already known why and expected that the t-distribution is a correct reflection of reality, both the original and the Crosshatch-Bandpass filtered Hutschenreuter map align with the expected t-distribution incredibly well, with the parameters for the t-distribution fit being similarly identical. As mentioned in section \ref{sec:FR_stats}, despite the clear visual incongruence, the important property of the t-distribution which appeared to not work with other distributions centred at zero, is the tailed prevalence. This ultimately gave such a small p-value in the Pearson's chi-squared test. Both distributions additionally are centred close to zero, with the systematic error being most likely a consequence of measurement bias or a consequence of the POSSUM dataset covering only a specific proportion of the sky. The latter can be seen in how the ISM's anisotropic distribution does not seem exactly symmetric in figure \ref{fig:anisotropy}, connecting it with the amount of RMs recorded by POSSUM in a specific quadrant of the southern hemisphere, seen in figure \ref{fig:rm_map}.


As mentioned, the results in this report regarding foreground techniques are results which may not immediately show utility in the next year or two. However, with more accurate interpolation techniques, like the one seen in \cite{ID58}, the pre-SKA and SKA-era analyses of the RM sky will greatly benefit from the preliminary research direction opened by this report.

\section{Magnetic Field Derivation}
\label{sec:mag_disc}

A key theme in this report is the distinction between validity and accuracy of a particular given method. As is common with first attempts as estimation, the lack of data and methodological development is apparent. This is the primary motivation for the report's focus on less on agreement and accuracy, but more on consideration of new methods and their mathematical validity. There is plenty of future work available, particularly in-terms of expanding upon the methodology of this report.

\subsection{Data Collection}
\label{ssec:B1}

The current POSSUM sample size is a major hinderance to achieving the stated research objectives. With the survey in its early stages, a significant proportion of the sky is yet to be covered. This does not prevent viable analysis from being performed, however it does limit the scope of the analysis, and can affect the accuracy of the evaluation. A big proportion of documented HVCs could not be analysed as they will not or currently are not covered by the POSSUM dataset as of May 2024.


The use of the HI sky map from \cite{ID6} is justified for the use of visual display and visual HVC detection, with the map being used to eliminate two HVCs for a lack of detection (HVC G282.3-38.3+117) and cartesian map distortions (HVC G298.0-81.7+127). However, the choice to use a HI map which filters out HI emission below a VLSR threshold (equivalent to a given column density threshold) can make it difficult to utilise the HI map for masking – a common method of isolating in-HVC and out-HVC RM populations. It is potentially difficult to do this even with a non-filtered HI map, as the boundary between what is and is not the HVC will always be difficult to discern. Masking would only be a first step, with the need to include RMs near but not on the HVC's HI boundary due to magnetic draping occurring outside the physical cloud itself. Any algorithm that can adapt itself to outline the specific shape of any given HVC and select the RMs within and around said outline would have to be highly complex and may even have to rely on new Artificial Intelligence techniques to account for the various shapes and sizes of HVCs.


The use of the Hutschenreuter map is justified given the use of interpolations in past research \citep{ID3, ID5, ID6, ID26, ID73}, and the analysis performed all throughout chapter \ref{cha:FR}. It is clear as well that the Hutschenreuter map can eliminate the most important source of foreground interference – that being the ISM. This is evidenced indirectly by figure \ref{fig:anisotropy}, with the elimination of the ISM's key feature being the innate anisotropy. It is expected that what remains after correction is still rather variant, as the interpolation is likely to add its own level of standard measurement error, and that the halo itself is magnetised to some degree, which is shown in the ridge-like structures in figure \ref{fig:all_hvcs}.


Removing the ridge-like structures is much more difficult and must be done after or during the calculation of the master RMs or line-of-sight magnetic field strengths. In concerns with determining a significant detection however, it is not of concern. This is because the HVC contribution to the RMs effectively 'overlaps' and adds to the background RM field, meaning that a detection can still be made even if the background interferes with the HVC. The exception to this rule is if the background is so noisy or strong, that any variation of RMs created by the HVC is eliminated. While this is rare, it can potentially bias results towards HVCs with notably higher line-of-sight magnetic field strengths, an effective form of Malmquist bias. Despite this, there is an argument to be made that HVCs which do not have significant contributions compared to the background should be thrown out in the calculation process, as there is naturally less evidence to verify if the HVC is experiencing magnetic draping, and that this phenomenon is calculable. Since most HVCs did have significant detections, however, it is not of concern to the outcome of specifically this report.


From sections \ref{sec:hvcs} and \ref{sec:sc}, and visually looking at HI profiles in figure \ref{fig:all_hvcs}, there is quite a variance in HVC size, albeit a smaller variance for HVCs in the CGM. The use of angular size to discriminate against HVCs that are abnormally large or small is reliant on the assumption that all HVCs are approximately the same distance away from the Earth as each other. Within an order of magnitude, this is assumedly true as all HVCs capable of being resolved are likely to be in the Galactic halo or nearby CGM \citep{ID66, ID74}. Additionally, HVCs that are much further away from or closer to the Galaxy are likely to be malformed in shape \citep{ID66} – a key example being the Smith Cloud \citep{ID28, ID64}. This means that these outlier HVCs are likely to have angular size distributions well outside $1-\pi$ degrees.


All these methods constrain the total amount of HVCs being analysed. Which is how a resulting 13 HVCs were selected. This limits the sample size, which can directly affect the quality of statistical analysis. Hence, an additional reason to label the findings of this report as a rough estimation.

\subsection{Neccesity of Assumptions}
\label{ssec:B2}

As mentioned in section \ref{sec:assumptions}, four major assumptions were employed in the analysis post-HVC selection and RM correction: (1) the Galactic halo is well-mixed; (2) the weighted average of a given HVC's HI column density is approximately equivalent to its peak value; (3) HVCs are approximately spherical; and (4) the RM contribution along the line of sight is an additive. There are both reasons for the necessity of these assumptions, and clear blind spots that they present.


While there is evidence that the Galactic halo has an ionisation fraction close to unity \citep{ID23}, the main reason for this assumption is that there is very little, and often conflicting evidence as to the true ionisation fraction of the halo, if it is homogeneous at all. There is also evidence to suggest that the gas surrounding HVCs themselves has very complex mixing patterns \cite{ID69}. The ridge-like structures found in the RM background of each HVC image is evidence to suggest a lack of homogeneity, however, it could be a consequence of turbulence captured at an instantaneous moment as well. Until there is better data on the ionisation fraction surrounding HVCs and in the halo itself, the results are likely to be inaccurate to within an order of magnitude. Note that by decreasing the ionisation fraction, the estimated magnetic field strengths become increasingly larger.


The assumption of homogeneity appears in the second assumption as well, with the core idea being that one can assume that the 'master RM' is effectively a measure of the RM passing through the centre of an ideally homogenous HVC. The need for this assumption is due to the lack of a method to truly find the correct HI outline of the HVC and the use of the HI map from \cite{ID6}. As well, the Moss catalogue only provides the peak HI column density, and all the RM data must be condensed into a single data point – thus a single-point analysis needs to be used regardless.  Statistically speaking, the weighted average is likely to be biased towards higher HI column densities due to Poisson noise, and given the equation from \cite{ID27}, equation \ref{eq:the_equation} – which relies on and approximates using this weighted average - this assumption appears to be much more reasonable than it may initially seem. The shortfall of this assumption comes from the use of the background overlap as correction, and the use of a circular selection window. With the background overlap acting as a correction to the HVC's 'master RM', it means that the HI column density calculation is also in-effect being applied to the RMs surrounding the HVC, which is certainly not an accurate representation of the HI column density in that region. The circular selection window also, as required, included RMs which surround but do not overlap with the HVC's HI profile, meaning that these RMs are analysed with the same column density as the RMs overlapping the HVC. Both issues are mitigated by the fact that the RMs are only converted to line-of-sight magnetic fields once a 'master RM' is created – the ideal being that issues surrounding electron density are dealt with once a single RM value has been reached.


The third and fourth assumptions are more trivial in-terms of their purpose. Without the use of masking, and with the requirement of a generalised algorithm to distinguish in-HVC and out-HVC populations, the most logical first step is to pick a simple shape to act as the outline. Given that HVCs are comet-like structures \citep{ID13}, it is also reasonable to estimate the main bulb of HVCs as a sphere. The shortfall of this assumption is simply in how it may not correctly distinguish RM populations in more malformed HVCs, which as mentioned, are less likely to appear due to the method of HVC selection chosen. The last assumption is used very often in past research \citep{ID27,ID3, ID26, ID5}. Thus, it is not of great concern in how it may affect the outcomes of this report, it is a fair approximation to make, given repeated amounts of correction for non-HVC interferences.


\subsection{Validity of Detection Methods}
\label{ssec:B5}

As mentioned, the Hutschenreuter map at mid-latitudes closely emulates the ideal of having variations on one square degree.  This may be problematic, as it means that the Hutschenreuter map may introduce variations in the corrected RM grid on such a scale. In fact, the ridge-like structures of the HVCs may be caused by spatial variations in interpolated data instead of it representing the true non-ISM foreground. This is why a visual comparison is made between the corrected RMs and the RMs post-average subtraction in figures \ref{fig:all_hvcs_avg} and \ref{fig:all_hvcs}. By taking an average of all the RMs within a neighbourhood of the HVC and subtracting it out and confirming that it looks visually like the corrected RMs, it can be demonstrated that the Hutschenreuter map is like subtraction using a constant factor i.e. the Hutschenreuter introduces no spatial variations because of interpolation. Two statistical tests in this report can further validate the qualitative analysis. Given that the Crosshatch-Bandpassed, and Raw, Hutschenreuter map is correlated significantly (as seen with the R-Squared statistic in table \ref{tab:fr_stats}), it means that the Hutschenreuter map is likely not creating additional spatial variations. It can also be seen that in the mid-latitudes in figure \ref{fig:colour_maps_1}, the linear correlation is stronger. The subtraction of a constant value also is equivalent to the uncorrected RMs in terms of the output of the KS test. Thus, by showing that the KS test simply is more selective for corrected RMs than it is for uncorrected RMs, it quantitatively verifies that these spatial variations are not significantly affecting the results. Note that while the KS test is more selective for corrected RMs, this does not imply an issue with spatial variation, but instead that the Hutschenreuter map is better at correcting the foreground than no foreground correction. Only in the vice versa case where the corrected and uncorrected KS tests retrieve uncorrelated or weaker results does it imply a problem with the Hutschenreuter map itself, as it would mean that spatial variations are introducing correlated interference.


Due to the lack of POSSUM data, even surrounding HVCs that overlap with the existing POSSUM dataset, there is a potential issue created from the Missing Completely at Random (MCAR) data. With this missing data, the full picture of foreground interferences or ridge-like structure variations cannot be determined and can cause the KS test to flag detections where detections may otherwise not be. While there are statistical methods to quantify the effect of MCAR data, such as Little's MCAR test \citep{ID72}, it is far too speculative to essentially predict what the full POSSUM dataset may look like and is much more worth waiting a few more years to repeat the analysis performed by this report.


From section \ref{sec:KStest}, there appears to be a weak correlation between the results of the KS test and the HVC's Galactic latitude ($\sim$0.6). Fortunately, this correlation is even weaker when comparing with the p-value ($\sim$0.4). Any level of correlation implies that the KS test itself is biased towards HVCs of a particular Galactic latitude, which can damage the credibility of detections made by the KS test. Part of this could be due to a low sample size or a limited Galactic latitude range, which may artificially increase the correlation – in this case there would be no bias. Another possibility may be that due to RMs at mid-latitudes being 'tamer', the KS test is more robust at determining a difference i.e. there is less noise in the calculated background – this case is what may introduce bias. Due to the small sample size of HVCs, whether the KS test is correlated with latitude cannot be determined, but it is important for future researchers to pay attention to this problem. There does not appear to be a problem with other HVC properties like VLSR or VGSR.


A main concern is if the ridge-like structures are a consequence of noise or spatially significant systematic error that is not caused by the interpolation itself. Or that the background noise may be different depending on the portion of sky being analysed. Typically, to remove the sky, astronomers would look at a random and isolated patch of sky to compare against. This method is not viable as the background population of each HVC is expected to be unique, and dissimilar to any arbitrarily sampled portion of the POSSUM dataset, due to the potential presence of RM foreground interferences overlapping with a specific HVC. Thus, there is no point in comparing the background population with a separate portion of RM sky.

\subsection{Validity of Derivation Methods}
\label{ssec:B3}

To further distinguish the differences between accuracy and validity is the notion of mathematical, statistical, and experimental validity. Because a method may be valid in a mathematical abstract does not mean that it may be experimentally valid. Mathematical derivations necessarily introduce small assumptions, on top of the major four listed in the report. These assumptions may cause the method to fail when put into practice. Here, there is an expectation that all three derivation methods (Variance Subtraction, KS-EDF, Weighted Mean) have specific points of failure for any given scenario. For example, the weighted mean mathematically underestimates for situations where the HVC is face-on. This is to such a predicted effect that in section \ref{sec:evaluation}, it was hypothesised that the weighted mean of RMs is likely to return bad data. This was confirmed false by the weighted mean having a reasonable magnetic field value range in figure \ref{fig:BBox} – although it does appear to return small values for some HVCs indicated by the lower quartile whisker of the box plot. The KS-EDF method overestimates due to the strongest deviation between populations likely occurring after a stationary point on the PDF, or inflexion point on the EDF, due to a delayed rise in the cumulative proportion. As a visual representation, note that the location of the x-statistic in figure \ref{fig:KSdiff} is off its nearest inflexion point by about $0.5-1$ $\mathrm{rad m}^{-2}$. All three tests in mathematically abstract and idealised situations work for most or some cases they may face. Which is why to statistically validate the results, a three-way method of analysis is required.


The use of the Weighted ANOVA test is crucial to the statistical validation of data. It allows for a three-way simultaneous comparison of the calculated values for each of the three methods while accounting for their uncertainties. If the Weighted ANOVA test returns a significance, it means that despite the apparent points of failure each method may have for a particular HVC, and the variations between each of the three methods, in general, the three methods agree with each other to some degree. Here, there is a weak agreement between each of the three methods – resulting in a 2-sigma confidence. This not as strong as a result as one may desire, but it is a starting point that demonstrates all three methods are headed in the right direction.


The use of the Tukey test (as seen in table \ref{tab:tukey_hsd}) is meant to compare each method with each other \citep{ID77, ID78}. This two-way comparison can provide greater insight into which methods are more likely to agree or disagree with each other. As from section \ref{sec:results}, it is quite apparent that the KS-EDF method is the worst performing of the three methods, which can be seen both visually and numerically. When comparing the KS-EDF method against the weighted mean, a 95\% confidence of statistically significant difference is obtained – which is particularly problematic given that the weighted mean is sanctioned by research \citep{ID5, ID26}. From this, it is reasonable to conclude that the KS-EDF method is the odd one out of the three and should be disregarded in future research. The variance subtraction, as far as this analysis is concerned, is statistically valid to some degree. However, more needs to be done to confirm this fact.


Despite the statistical confirmation of validity, there is ultimately no way to determine the experimental validity of each of these three methods without modelling. This is a recommended direction for cementing the results of the report.


As expected, from table \ref{tab:Bdev} the uncertainties are relatively large. There is no decent way at visually representing this statement due to how cluttered any given graph would be. The large uncertainties are expected. Certainly, for both variance subtraction and weighted mean, the calculation of uncertainties are mathematically justified. It was much more difficult to obtain KS-EDF uncertainties, leading to a potential over-estimation of error. This only further supports the conclusion that the KS-EDF method is a 'dead end'.


The methodologies proposed in chapter \ref{sec:evaluation} play a role in how researchers will estimate the effect of magnetic draping, with the clear requirement of more techniques or 'angles of attack' to solving the problem of calculation for an arbitrary HVC, of arbitrary orientation, size, shape, and angular size. Effectively the goal of developing a generalised algorithm as discussed in section \ref{sec:outline} is necessary to allow for astronomers to mass-estimate magnetic draping. An important caveat being that the four major assumptions being reduced in their contribution to systematic or random errors.

