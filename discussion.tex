\chapter{Discussion}
\label{cha:discussion}

Like the rest of the report structure, this section is split under the two different research questions investigated in this thesis, starting with the secondary goal of the report and finishing with the primary goals of the report.

\section{Foreground Removal}
\label{sec:fr_disc}

A major shortfall of the foreground subtraction investigation is the lack of statistical analysis, which will be a common theme throughout this section. In some cases, this is justified, as statistical analysis in not needed if, by eye, the method clearly does not work. At the same time, this investigation is limited from a lack of characterisation of the true RM sky, including simulations to predict the distribution of corrected RMs and high-quality data to interpolate with.


This report is not meant to act as a definitive conclusion on research into foreground subtraction techniques, but to point out important considerations to make as research moves into the SKA era, and to conduct a preliminary analysis of how to tackle these predicted problems as outlined in section \ref{sec:intp}. Section \ref{sec:intp}, highlights how higher-definition data will inevitably result in issues involving foreground correction for objects of a particular angular size. Thus, the need to extend beyond interpolation techniques when conducting SKA-era analysis.


The NUFFT method is one example of a method that can be assessed visually, and without the need for statistical analysis. Figure \ref{fig:nufft} clearly shows that even carrying out an interpolation using a NUFFT requires a large sample size to be adequate. It would not be appropriate to completely write-off NUFFTs as an interpolation and bandpass technique, as with a more complete POSSUM dataset, and future SKA RM projects, millions of RM points would be adequate to interpolate and bandpass using a NUFFT.


Given that the Hutschenreuter map interpolates data that is at a spatial sampling frequency of 1 sampling point per degree squared, attempting to remove objects on the scale of 1 degree should not alter the interpolation at all. However, there are several factors that prevent a perfect correlation between pre and post analyses: H-alpha inclusion providing more detailed information; more RM sources being detected by telescopes close to the Galactic midplane; and distortion effects occurring on the edges of the cartesian version of the map. The latter two can be accounted for by performing the filtering on a reduced form of the dataset that does not include extremes in Galactic latitude. Additionally, from figure \ref{fig:rm_scatter}, it is clear that RMs near the Galactic midplane can become quite scattered. Even though this is true for the Hutschenreuter map, it still is a demonstration of the scatter present near the Galactic midplane.


This is the justification for the use of a Pearson's R-squared statistic for correlation, as it can determine how similar the pre and post bandpass and convolution methods, as discussed throughout section \ref{sec:FR_stats}, are at preserving the broader RM sky. The expectation is that the R-squared statistic is close to unity but not exactly unity. One major downside is that there is no way to quantify the appropriate R-squared statistic that should be desired. Thus, only a high R-squared statistic is possible qualifier.


It is clear from the R-squared statistics from table \ref{tab:fr_stats} that, even with a less-exact method of determining validity, both the Crosshatch-Bandpass and Annulus-Bandpass methods are viable as an interpolation correction method. It can be pointed out that it may be unfair to compare the Annulus-Convolved method to the raw interpolation. However, it is justified in the sense that the raw interpolation has been used as the primary method in past literature, and from figure \ref{fig:anisotropy}, it is demonstrably useful at removing the anisotropy produced by the ISM.


The Annulus-Bandpass method should not immediately be adopted as a method without further experimentation. The Crosshatch-Bandpass method is another viable method that should not be discounted. However, with the rippling effects shown in figure \ref{fig:ripples}, there are improvements that can be made. The Fourier Transform of a top-hat function is inevitably going to introduce rippling. This may not be as much of the case with the Tukey or Gaussian windows, which is much more common in other signal processing analyses. The choice to reduce the bandpass opacity is another option that clearly removes ripples, at the cost of dampening the intended goal of using FFT signal processing methods. Some combination of these may be useful for future research relying on RM-correction.


The interesting result provided in \cite{ID73} is that corrected RMs follow a t-distribution, still showing with a repeated analysis on the Hutschenreuter map. There is still little reason for why corrected RMs take a t-distribution shape, or if this is meant to be the true distribution of RMs post ISM correction. It is rather crucial that research be conducted into what the true distribution of ISM-corrected RMs should be. It is trivial to understand why the distribution should be symmetrical and centred at zero, as extragalactic polarisation is effectively random and not biased based on sign. The Student's t-distribution is one of many different distributions satisfying these properties that are viable candidates for representing corrected RMs, it could be that the central limit theorem applies here but that there is a subpopulation of RMs that are intrinsically higher \citep{ID1}.


Under the assumption that it is already known why that the t-distribution is a correct reflection of reality, both the original and the bandpass filters of the Hutschenreuter map align with the expected t-distribution incredibly well, as seen from table \ref{tab:fr_stats}, with the parameters for the t-distribution fit being similarly identical. As mentioned in section \ref{sec:FR_stats}, despite the clear visual incongruence, the important property of the t-distribution which appeared to not work with other distributions centred at zero, is the tailed prevalence. This ultimately gave a small p-value in the Pearson's chi-squared test. Both distributions additionally are centred close to zero, with the systematic error being most likely a consequence of measurement bias or a consequence of the POSSUM dataset covering only a specific proportion of the sky. The latter can be seen in how the ISM's anisotropic distribution does not seem exactly symmetric in figure \ref{fig:anisotropy}, connecting it with the amount of RMs recorded by POSSUM in a specific quadrant of the southern hemisphere, seen in figure \ref{fig:rm_map}. This is speculation, as any higher-order structure can create the same effect.


The results in this report regarding foreground techniques are results which may not immediately show utility in the next year or two. However, with more accurate interpolation techniques, like the one seen in \cite{ID58}, the pre-SKA and SKA-era analyses of the RM sky will greatly benefit from the preliminary research direction opened by this report.

\section{Magnetic Field Derivation}
\label{sec:mag_disc}

A key theme in this report is the distinction between validity and accuracy of a particular given method. As is common with first attempts at estimation, the lack of data and methodological development is apparent. This is the primary motivation for the report's focus - less on agreement and accuracy, but more on consideration of new methods and their mathematical validity. There is plenty of future work available, particularly in-terms of expanding upon the methodology of this report. The way this section is sturctured aims to step through each component of the process in the analysis of results: the limitations of the data from the onset; a justification of the assumptions employed in both detection and derivation; an assesment of the detection and derivation methods themselves; and lastly the implications this has on broader research.

\subsection{Limitations of Collected Data}
\label{ssec:B1}

The current POSSUM sample size is a major hinderance to achieving the stated research objectives. With the survey in its early stages, a significant proportion of the sky is yet to be covered. This does not prevent viable analysis from being performed, however it does limit the scope of the analysis, and can affect the accuracy of the evaluation. A large proportion of documented HVCs could not be analysed.


The use of the HI sky map from \cite{ID6} is justified for the use of visual display and visual HVC detection, with the map being used to eliminate two HVCs for: a lack of detection (HVC G282.3-38.3+117); and cartesian map distortions (HVC G298.0-81.7+127). However, the choice to use a HI map which filters out HI emission below a VLSR threshold (equivalent to a given column density threshold) can make it difficult to utilise the HI map for masking – a common method of isolating in-HVC and out-HVC RM populations. It is potentially difficult to do this even with a non-filtered HI map, as the boundary between what is and is not the HVC will always be difficult to discern. Masking would only be a first step, with the need to include RMs near but not on the HVC's HI boundary due to magnetic draping occurring outside the physical cloud itself. Any algorithm that can adapt itself to outline the specific shape of any given HVC and select the RMs within and around said outline would have to be highly complex.


The use of the Hutschenreuter map is justified given the use of interpolations in past research \citep{ID3, ID5, ID6, ID26, ID73}, and the correlation analysis performed from table \ref{tab:fr_stats}. It is clear as well that the Hutschenreuter map can eliminate the most important source of foreground interference – that being the ISM. This is evidenced indirectly by figure \ref{fig:anisotropy}, with the elimination of the ISM's key feature being its innate anisotropy \citep{ID30}. It is expected that what remains after correction is a more accurate depiction of the magnetic fields of the halo and halo objects, which is shown in the ridge-like structures in figure \ref{fig:all_hvcs}. However the interpolation is likely to add its own level of standard measurement error.


Removing the ridge-like structures from figure \ref{fig:all_hvcs} is neccesary, as the halo itself is further random interference that is smoothed out via interpolation. This is why the term "background" is used in distinction to "foreground", as the halo along the line-of-sight surrounds the HVCs. It is much more difficult to remove this and must be done during the calculation of the line-of-sight magnetic field strengths. When determining a significant detection however, it is not of concern. This is because the HVC's contribution to the RMs are superimposed onto the background RM field, meaning that a detection can still be made even if the background interferes with the HVC. The exception to this rule is if the background is so noisy or strong, that any variation of RMs created by the HVC is eliminated. While this is rare, it can potentially bias results towards HVCs with notably higher line-of-sight magnetic field strengths, an effective form of Malmquist bias. Despite this, HVCs which do not have significant contributions compared to the background should not be considered in the estimation process, as there is naturally less evidence to verify if the HVC is experiencing magnetic draping. Since most HVCs did have significant detections from the p-values in table \ref{tab:KStest}, however, it is not of concern to the outcome of specifically this report.


Figure \ref{fig:all_hvcs} and \cite{ID3} shows that HVCs come in a large variety of sizes, and so the selection criteria was designed to ensure the RM analysis and foreground correction was tractable. Despte it necessarily limiing the generality of the conclusions. As RM analysis techniques mature in future, a more representative sample of HVCs can be studied.


\subsection{Neccesity of Assumptions}
\label{ssec:B2}

As mentioned in section \ref{sec:assumptions}, four major assumptions were employed in the analysis, post-HVC selection and RM correction: (1) the Galactic halo is well-mixed; (2) the weighted average of a given HVC's HI column density is approximately equivalent to its peak value; (3) HVCs are approximately spherical; and (4) the RM contribution along the line of sight is an additive. There are both reasons for the necessity of these assumptions, and the blind spots that they present.


While there is evidence that the Galactic halo has an ionisation fraction close to unity \citep{ID23}, the main reason for this being labelled an assumption is that there is very little evidence demonstrating this, if the halo is homogeneous at all. There is evidence to suggest that the gas surrounding HVCs themselves have very complex mixing patterns \cite{ID69, ID4}. The ridge-like structures found in the RM background of each HVC image is evidence to suggest a lack of homogeneity, however, it could be a consequence of turbulence captured at an instantaneous moment as well. Until there is better data on the ionisation fraction surrounding HVCs and in the halo itself, the results from table \ref{tab:Bdev} are likely to be inaccurate to within an order of magnitude. Note that by decreasing the ionisation fraction, the estimated magnetic field strengths become increasingly larger, meaning that changing the ionisation fraction would imply even larger magnetic field values, deviating even further from simulation predictions \citep{ID24}.


The second assumption exists due to limited information. That being, a difficulty to morphologically analyse HVCs given their HI profiles from \cite{ID6}, with the only other data being the peak HI column density from the Moss catalogue. HI data received by telescopes, even filtering for VLSR, is limited by Poisson noise. This naturally leads to higher HI values being weighted more in a weighted average. Equation \ref{eq:the_equation} from \cite{ID27} utilises the weighted average HI column density, so it is somewhat reasonable to say that weighted HI column density is biased towards the peak value. The use of a single number is at least mathematically valid from the equation \ref{eq:the_equation}. Before applying equation \ref{eq:the_equation}, a single 'master RM' value for the HVC is constructed from the statistical aggregation of background and in-HVC RMs. This ensures that the HVC's master RM is the best representation of the RM contribution of only that HVC. The issue is that the signals from background RMs do not traverse the same HI region, and thus the respective column density would have starkly different values. If we were to convert each RM source to magnetic field before combining them into a singular value, a completely different value would be achieved. This may introduce error in the calculation of the magnetic field as it effectively homogenises the HI column density across the HVC image. The idea is that this concern is counteracted by the fact that the background is only used as a tool to isolate the HVC's RM contribution, so that by subtracting the background RM aggregate we remove any potential influence from the background magnetic fields. 



The third and fourth assumptions are more trivial in-terms of their purpose. Without the use of masking, and with the requirement of a generalised algorithm to distinguish in-HVC and out-HVC populations, the most logical first step is to pick a simple shape to act as the outline. Given that HVCs are comet-like structures \citep{ID13}, it is also reasonable to estimate the main bulb of HVCs as a sphere. The shortfall of this assumption is simply in how it may not correctly distinguish RM populations in more malformed HVCs, which as mentioned, are less likely to appear due to the method of HVC selection chosen. The last assumption is used very often in past research \citep{ID27,ID3, ID26, ID5}. Thus, it is not of great concern in how it may affect the outcomes of this report. It is a fair approximation to make, given that most non-HVC RM contributions are corrected for.


\subsection{Validity of Detection Methods}
\label{ssec:B5}

As mentioned, the Hutschenreuter map at mid-latitudes ($20^\circ<|b|<80^\circ$) can have variations on one square degree. This may be problematic, as it means that the Hutschenreuter map may introduce variations in the corrected RM grid on such a scale. In fact, the ridge-like structures of the background may be caused by spatial variations in interpolated data instead of it representing the true non-ISM RM contributions. This is why a visual comparison is made between the corrected RMs and the RMs post-average subtraction in figures \ref{fig:all_hvcs_avg} and \ref{fig:all_hvcs}. By taking an average of all the RMs within a neighbourhood of the HVC, subtracting it out, and confirming that it looks visually like the corrected RMs, it can be demonstrated that the Hutschenreuter map is like subtraction using a constant factor i.e. the Hutschenreuter map introduces no spatial variations because of interpolation. Two statistical tests in this report can further validate the qualitative analysis from figure \ref{fig:all_hvcs_avg}. The Crosshatch-Bandpassed, and raw, Hutschenreuter map is correlated significantly (as seen with the R-Squared statistic in table \ref{tab:fr_stats}). This means that the Hutschenreuter map is likely not creating additional spatial variations. It can also be seen that in the mid-latitudes in figure \ref{fig:colour_maps_1}, the linear correlation is stronger.


The subtraction of a constant value, that being the mean of all RMs in the field, would not alter the KS statistic or the p-value when the KS test is applied to uncorrected RMs. In addition to this, the KS test on uncorrected RMs from table \ref{tab:KStest} consistently has smaller p-values in all cases, meaning that this KS test is less selective than the corrected-RM KS test. Thus it can be quantitatively verified that these spatial variations are not significantly affecting the results. Note that while the KS test is more selective for corrected RMs, this implies that the Hutschenreuter map is better at correcting the foreground than no foreground correction at all, not that it introduces its own spatial interference.


Due to the current patchy coverage of POSSUM, even surrounding HVCs that overlap with the existing POSSUM dataset, there is a potential issue created from the Missing Completely at Random (MCAR) data. With this missing data, the full picture of foreground interferences or ridge-like structure variations cannot be determined and can cause the KS test to flag significant differences (detections) where detections may otherwise not be. While there are statistical methods to quantify the effect of MCAR data, such as Little's MCAR test \citep{ID72}, it is far too speculative to essentially predict what the full POSSUM dataset may look like and is much more worth waiting a few more years to repeat the analysis performed by this report.


From section \ref{sec:KStest}, there appears to be a moderate correlation between the results of the KS test and the HVC's Galactic latitude ($\sim$0.6). Fortunately, this correlation is weaker when comparing the p-value with Galactic lattitude ($\sim$0.4). Any level of correlation implies that the KS test itself is biased towards HVCs of a particular Galactic latitude, which can damage the credibility of detections made by the KS test. Part of this could be due to a low sample size or a limited Galactic latitude range, which may artificially increase the correlation. Another possibility may be that due to RMs at mid-latitudes being less scattered and disorted, the KS test is more robust at determining a difference i.e. there is less noise in the calculated background – this case is what may introduce bias. Due to the small sample size of HVCs, whether the KS test is related to latitude cannot be determined, but it is important for future researchers to pay attention to this problem. There does not appear to be a correlation with other HVC properties like VLSR or VGSR.

\subsection{Validity of Derivation Methods}
\label{ssec:B3}

To further distinguish the differences between accuracy and validity, there is the notion of mathematical, statistical, and experimental validity. Because a method may be valid in a mathematical abstract does not mean that it may be experimentally valid. Mathematical derivations necessarily introduce small assumptions, on top of the major four listed in the report. These assumptions may cause the method to fail when put into practice. Here, there is an expectation that all three derivation methods (Variance Subtraction, KS-EDF, Weighted Mean) have specific points of failure for any given scenario. For example, the weighted mean mathematically underestimates for situations where the HVC is face-on, as explained by the coffee-stain analogy in section \ref{sec:evaluation}. This idea is doubtable given the weighted mean having a reasonable magnetic field value range in figure \ref{fig:BBox} – although it does appear to return small values for some HVCs indicated by the lower quartile whisker of the box plot, and the uncertainties. The KS-EDF method overestimates due to the strongest deviation between populations likely occurring after a stationary point on the PDF, or inflexion point on the EDF, due to a delayed rise in the cumulative proportion. As a visual representation, note that the location of the x-statistic in figure \ref{fig:KSdiff} is off its nearest inflexion point by about $0.5-1$ $\mathrm{rad m}^{-2}$. All three tests in mathematically abstract and idealised situations work for most cases they may face. This is why to statistically validate the results, a three-way method of analysis is required.


The use of the Weighted ANOVA test is crucial to the statistical validation of data. It allows for a three-way simultaneous comparison of the calculated values for each of the three methods while accounting for their uncertainties. If the Weighted ANOVA test returns a significance, it means that despite the apparent points of failure each method may have for a particular HVC, and the variations between each of the three methods, in general, the three methods agree with each other to some degree. Here, there is a weak agreement between each of the three methods – resulting in a 2-sigma confidence. This not as strong as a result as one may desire, but it is a starting point that demonstrates all three methods are headed in the right direction.


The use of the Tukey test (as seen in table \ref{tab:tukey_hsd}) is meant to compare each method with each other \citep{ID77, ID78}. This two-way comparison can provide greater insight into which methods are more likely to agree or disagree with each other. As from section \ref{sec:results}, it is quite apparent that the KS-EDF method is the worst performing of the three methods, which can be seen both visually and numerically. When comparing the KS-EDF method against the weighted mean, a 95\% confidence of a statistically significant difference is obtained – which is particularly problematic given that the weighted mean is sanctioned by research \citep{ID5, ID26}. From this, it is reasonable to conclude that the KS-EDF method is the odd one out of the three and should be disfavoured in future research. Comparing the weighted mean and variance subtraction using the reduced chi-squared statistic produced a very well-aligned goodness of fit (i.e. close to unity).


Despite the statistical confirmation of validity, there is ultimately no way to determine the experimental validity of each of these three methods without modelling. This is a recommended direction for cementing the results of the report. As expected, from table \ref{tab:Bdev} the uncertainties are relatively large. There is no decent way at visually representing this statement due to how cluttered any given graph would be. The large uncertainties are expected. Certainly, for both variance subtraction and weighted mean, the calculation of uncertainties are mathematically justified. It was more difficult to obtain KS-EDF uncertainties, leading to a potential over-estimation of error. This only further supports the conclusion that the KS-EDF method is disfavourable.


For all three methods, some of the uncertainties in table \ref{tab:Bdev} imply a nonexistent magnetic field within one sigma. It is important to seperate the KS test from the estimation techniques. As mentioned in section \ref{ssec:results_uncertainties}, if the estimation produces such large uncertainies, it does not imply a lack of detection - instead it implies that the evaluation method is imprecise - something anticipated by the assumptions laid out in section \ref{sec:assumptions}. All three methods are non-physical and not designed to test for the prescence of signature unlike the KS test. At the same time, it has been highlighted that each method can have multiple points of faliure like as discussed with the weighted mean in section \ref{sec:evaluation}, where by some consequence of morphology and orientation, the evaluation returns zero despite there clearly being a unique signature. This is the primary factor that influences the uncertainties to be so large, as without a paring complex morphological analysis, a precise estimate of value is unacheivable, despite the clear marking of a signature from a simple statistical comparison.

\subsection{Implications of the Results}
\label{ssec:B4}

The methodologies proposed in chapter \ref{sec:evaluation} play a role in how researchers will estimate the effect of magnetic draping, with the clear requirement of more techniques or 'angles of attack' to solving the problem of calculation for an arbitrary HVC, of arbitrary orientation, size, shape, and angular size. Effectively the goal of developing a generalised algorithm as discussed in section \ref{sec:outline} is necessary to allow for astronomers to estimate magnetic draping. An important caveat being that the four major assumptions being reduced in future contributions to systematic or random errors.


The implications of the magnetic draping hypothesis have significant ramifications on the future of galactic archaeological research. It would necessarily mean that HVCs are protected by magnetic fields, which confirms a lot of \textit{qualitative} conclusions of past research done on modelling and predictions of HVC survivability (\citeauthor{ID23} simulations). It additionally aligns with observations of the Smith Cloud \citep{ID5, ID26}. While it confirms the efforts made by past research, it can help bring astronomers one step closer to understanding the origins of pristine gas for star-forming galaxies, with a proven method for how galaxies can obtain said gas. A small proportion of HVCs that did not have significant magnetic field detections – specifically HVCs G089.0-64.7-311 and G248.9+36.8+181. If this were not caused by methodological error, a re-examination of both HVCs is required.


While the conclusions point towards magnetic draping signatures to three sigma confidence, the order of magnitude higher magnetic field values is of notable interest. This conflicts with precious simulational data such as in the \citeauthor{ID23} simulations. It also implies that the Smith Cloud is less exceptional of a HVC than initially beleived. This order of magnitude increase should result in other consequences - most notably both Rayleigh-Taylor instabilites in the HVC gas and a significant slowing effect of HVCs due to magnetic pressure. The former of these could explain the complex mixing around HVCs \citep{ID67}. The outcomes of the methods extend beyond HVCs, but on other astronomical objects, such as in molecular clouds or magnetised HI superbubbles \citep{ID70,ID75}. It is still clear that these conclusions cannot be definitive due to the size of the uncertainies calculated.